{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Limpeza e Filtragem\n",
    "O Problema: Temos gigabytes de dados misturados (Renda Fixa, Cambial, Multimercado). O desafio pede especificamente a \"classe de A√ß√µes\".\n",
    "\n",
    "Script:\n",
    "\n",
    "1 - Ler o arquivo de Cadastro (cad_fi.csv) para pegar os CNPJs que s√£o de \"A√ß√µes\".\n",
    "2 - Ler os arquivos mensais (inf_diario...) um por um.\n",
    "3 - Filtrar apenas as linhas que t√™m esses CNPJs.\n",
    "4 - Salvar tudo num arquivo √∫nico e otimizado (formato Parquet, que √© muito mais r√°pido que CSV para leitura posterior)."
   ],
   "id": "7a547bfdfd264243"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Importamos as bibliotecas e definimos os caminhos. Usaremos o cad_fi.csv para filtrar os fundos de interesse antes de processar as s√©ries temporais pesadas.",
   "id": "3cebf741fc8b773c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Imports e Configura√ß√£o\n",
    "Importamos as bibliotecas e definimos os caminhos. Usaremos o cad_fi.csv para filtrar os fundos de interesse antes de processar as s√©ries temporais pesadas."
   ],
   "id": "1d5717752085d2ae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-20T00:23:16.338333300Z",
     "start_time": "2025-12-20T00:23:16.327669600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import gc # Garbage Collector para limpar mem√≥ria RAM\n",
    "\n",
    "# Caminhos\n",
    "RAW_DIR = '../data/raw'\n",
    "PROCESSED_DIR = '../data/processed'\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)"
   ],
   "id": "c9008164f4aed950",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Filtrando os CNPJs de A√ß√µes\n",
    "\n",
    "Segundo o escopo do desafio, devemos utilizar a classe de A√ß√µes. Carregamos o cadastro (cad_fi.csv), filtramos por CLASSE == 'Fundo de A√ß√µes' e status EM FUNCIONAMENTO NORMAL. Isso gera uma lista de CNPJs v√°lidos (\"White List\") para aplicarmos nos dados di√°rios."
   ],
   "id": "f5bb235c115ff8d9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-20T00:23:16.676984100Z",
     "start_time": "2025-12-20T00:23:16.340848Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"üìÇ Carregando cadastro de fundos...\")\n",
    "\n",
    "# L√™ o cadastro (encoding 'latin1' √© padr√£o de arquivos governamentais antigos no BR)\n",
    "df_cad = pd.read_csv(f'{RAW_DIR}/cad_fi.csv', sep=';', encoding='latin1', low_memory=False)\n",
    "\n",
    "# Normaliza colunas para evitar erros de mai√∫sculas/min√∫sculas\n",
    "df_cad.columns = df_cad.columns.str.upper()\n",
    "\n",
    "# FILTRO 1: Apenas classe 'A√ß√µes'\n",
    "# Obs: Na CVM, a classe exata geralmente vem como 'Fundo de A√ß√µes'\n",
    "filtro_acoes = df_cad['CLASSE'] == 'Fundo de A√ß√µes'\n",
    "\n",
    "# FILTRO 2: Apenas fundos em funcionamento (opcional, mas remove lixo)\n",
    "filtro_ativo = df_cad['SIT'] == 'EM FUNCIONAMENTO NORMAL'\n",
    "\n",
    "# Aplica filtros\n",
    "df_acoes = df_cad[filtro_acoes & filtro_ativo].copy()\n",
    "\n",
    "# Cria a lista de CNPJs permitidos (nossa \"White List\")\n",
    "cnpjs_acoes = set(df_acoes['CNPJ_FUNDO'].unique())\n",
    "\n",
    "print(f\"‚úÖ Total de fundos de A√ß√µes encontrados: {len(cnpjs_acoes)}\")\n",
    "print(f\"Exemplos de Classes no arquivo: {df_cad['CLASSE'].unique()[:5]}\")"
   ],
   "id": "4876f79ac4c42767",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Carregando cadastro de fundos...\n",
      "‚úÖ Total de fundos de A√ß√µes encontrados: 0\n",
      "Exemplos de Classes no arquivo: [nan 'Multimercado' 'A√ß√µes' 'Renda Fixa' 'Referenciado']\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Loop de Processamento (O \"Triturador\")\n",
    "\n",
    "Agora iteramos sobre todos os arquivos mensais baixados. Para cada arquivo:\n",
    "\n",
    "1 - Carregamos o CSV na mem√≥ria.\n",
    "2 - Filtramos mantendo apenas os CNPJs de A√ß√µes identificados no passo anterior.\n",
    "3 - Acumulamos o resultado.\n",
    "\n",
    "Otimiza√ß√£o: Usamos gc.collect() para liberar mem√≥ria RAM entre as itera√ß√µes, evitando que o computador trave com o volume de dados."
   ],
   "id": "293f6e67707a912c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-20T00:23:27.933739800Z",
     "start_time": "2025-12-20T00:23:16.678982800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Lista todos os arquivos de informe di√°rio ordenados\n",
    "arquivos_diarios = sorted(glob.glob(f'{RAW_DIR}/inf_diario_fi_*.csv'))\n",
    "\n",
    "dfs_filtrados = []\n",
    "\n",
    "print(f\"üöÄ Iniciando processamento de {len(arquivos_diarios)} arquivos mensais...\")\n",
    "\n",
    "for arquivo in arquivos_diarios:\n",
    "    print(f\"Processando: {os.path.basename(arquivo)}...\")\n",
    "\n",
    "    try:\n",
    "        # L√™ o arquivo mensal\n",
    "        df_mes = pd.read_csv(arquivo, sep=';', encoding='latin1', low_memory=False)\n",
    "\n",
    "        # Filtra apenas os CNPJs de A√ß√µes\n",
    "        df_mes = df_mes[df_mes['CNPJ_FUNDO'].isin(cnpjs_acoes)]\n",
    "\n",
    "        # Adiciona √† lista se sobrou alguma coisa\n",
    "        if not df_mes.empty:\n",
    "            dfs_filtrados.append(df_mes)\n",
    "\n",
    "        # Limpeza de mem√≥ria\n",
    "        del df_mes\n",
    "        gc.collect()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao ler {arquivo}: {e}\")\n",
    "\n",
    "print(\"üîÑ Concatenando todos os meses...\")\n",
    "df_final = pd.concat(dfs_filtrados, ignore_index=True)\n",
    "\n",
    "print(f\"üìä Dataset Final: {df_final.shape[0]} linhas e {df_final.shape[1]} colunas.\")"
   ],
   "id": "1d17197d0ae80dfb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Iniciando processamento de 25 arquivos mensais...\n",
      "Processando: inf_diario_fi_202312.csv...\n",
      "‚ùå Erro ao ler ../data/raw\\inf_diario_fi_202312.csv: 'CNPJ_FUNDO'\n",
      "Processando: inf_diario_fi_202401.csv...\n",
      "‚ùå Erro ao ler ../data/raw\\inf_diario_fi_202401.csv: 'CNPJ_FUNDO'\n",
      "Processando: inf_diario_fi_202402.csv...\n",
      "‚ùå Erro ao ler ../data/raw\\inf_diario_fi_202402.csv: 'CNPJ_FUNDO'\n",
      "Processando: inf_diario_fi_202403.csv...\n",
      "‚ùå Erro ao ler ../data/raw\\inf_diario_fi_202403.csv: 'CNPJ_FUNDO'\n",
      "Processando: inf_diario_fi_202404.csv...\n",
      "‚ùå Erro ao ler ../data/raw\\inf_diario_fi_202404.csv: 'CNPJ_FUNDO'\n",
      "Processando: inf_diario_fi_202405.csv...\n",
      "‚ùå Erro ao ler ../data/raw\\inf_diario_fi_202405.csv: 'CNPJ_FUNDO'\n",
      "Processando: inf_diario_fi_202406.csv...\n",
      "‚ùå Erro ao ler ../data/raw\\inf_diario_fi_202406.csv: 'CNPJ_FUNDO'\n",
      "Processando: inf_diario_fi_202407.csv...\n",
      "‚ùå Erro ao ler ../data/raw\\inf_diario_fi_202407.csv: 'CNPJ_FUNDO'\n",
      "Processando: inf_diario_fi_202408.csv...\n",
      "‚ùå Erro ao ler ../data/raw\\inf_diario_fi_202408.csv: 'CNPJ_FUNDO'\n",
      "Processando: inf_diario_fi_202409.csv...\n",
      "‚ùå Erro ao ler ../data/raw\\inf_diario_fi_202409.csv: 'CNPJ_FUNDO'\n",
      "Processando: inf_diario_fi_202410.csv...\n",
      "‚ùå Erro ao ler ../data/raw\\inf_diario_fi_202410.csv: 'CNPJ_FUNDO'\n",
      "Processando: inf_diario_fi_202411.csv...\n",
      "‚ùå Erro ao ler ../data/raw\\inf_diario_fi_202411.csv: 'CNPJ_FUNDO'\n",
      "Processando: inf_diario_fi_202412.csv...\n",
      "‚ùå Erro ao ler ../data/raw\\inf_diario_fi_202412.csv: 'CNPJ_FUNDO'\n",
      "Processando: inf_diario_fi_202501.csv...\n",
      "‚ùå Erro ao ler ../data/raw\\inf_diario_fi_202501.csv: 'CNPJ_FUNDO'\n",
      "Processando: inf_diario_fi_202502.csv...\n",
      "‚ùå Erro ao ler ../data/raw\\inf_diario_fi_202502.csv: 'CNPJ_FUNDO'\n",
      "Processando: inf_diario_fi_202503.csv...\n",
      "‚ùå Erro ao ler ../data/raw\\inf_diario_fi_202503.csv: 'CNPJ_FUNDO'\n",
      "Processando: inf_diario_fi_202504.csv...\n",
      "‚ùå Erro ao ler ../data/raw\\inf_diario_fi_202504.csv: 'CNPJ_FUNDO'\n",
      "Processando: inf_diario_fi_202505.csv...\n",
      "‚ùå Erro ao ler ../data/raw\\inf_diario_fi_202505.csv: 'CNPJ_FUNDO'\n",
      "Processando: inf_diario_fi_202506.csv...\n",
      "‚ùå Erro ao ler ../data/raw\\inf_diario_fi_202506.csv: 'CNPJ_FUNDO'\n",
      "Processando: inf_diario_fi_202507.csv...\n",
      "‚ùå Erro ao ler ../data/raw\\inf_diario_fi_202507.csv: 'CNPJ_FUNDO'\n",
      "Processando: inf_diario_fi_202508.csv...\n",
      "‚ùå Erro ao ler ../data/raw\\inf_diario_fi_202508.csv: 'CNPJ_FUNDO'\n",
      "Processando: inf_diario_fi_202509.csv...\n",
      "‚ùå Erro ao ler ../data/raw\\inf_diario_fi_202509.csv: 'CNPJ_FUNDO'\n",
      "Processando: inf_diario_fi_202510.csv...\n",
      "‚ùå Erro ao ler ../data/raw\\inf_diario_fi_202510.csv: 'CNPJ_FUNDO'\n",
      "Processando: inf_diario_fi_202511.csv...\n",
      "‚ùå Erro ao ler ../data/raw\\inf_diario_fi_202511.csv: 'CNPJ_FUNDO'\n",
      "Processando: inf_diario_fi_202512.csv...\n",
      "‚ùå Erro ao ler ../data/raw\\inf_diario_fi_202512.csv: 'CNPJ_FUNDO'\n",
      "üîÑ Concatenando todos os meses...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 30\u001B[39m\n\u001B[32m     27\u001B[39m         \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m‚ùå Erro ao ler \u001B[39m\u001B[38;5;132;01m{\u001B[39;00marquivo\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     29\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33müîÑ Concatenando todos os meses...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m30\u001B[39m df_final = \u001B[43mpd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mconcat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdfs_filtrados\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m     32\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33müìä Dataset Final: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdf_final.shape[\u001B[32m0\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m linhas e \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdf_final.shape[\u001B[32m1\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m colunas.\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\core\\reshape\\concat.py:382\u001B[39m, in \u001B[36mconcat\u001B[39m\u001B[34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001B[39m\n\u001B[32m    379\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m copy \u001B[38;5;129;01mand\u001B[39;00m using_copy_on_write():\n\u001B[32m    380\u001B[39m     copy = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m382\u001B[39m op = \u001B[43m_Concatenator\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    383\u001B[39m \u001B[43m    \u001B[49m\u001B[43mobjs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    384\u001B[39m \u001B[43m    \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m=\u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    385\u001B[39m \u001B[43m    \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[43m=\u001B[49m\u001B[43mignore_index\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    386\u001B[39m \u001B[43m    \u001B[49m\u001B[43mjoin\u001B[49m\u001B[43m=\u001B[49m\u001B[43mjoin\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    387\u001B[39m \u001B[43m    \u001B[49m\u001B[43mkeys\u001B[49m\u001B[43m=\u001B[49m\u001B[43mkeys\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    388\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlevels\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlevels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    389\u001B[39m \u001B[43m    \u001B[49m\u001B[43mnames\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnames\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    390\u001B[39m \u001B[43m    \u001B[49m\u001B[43mverify_integrity\u001B[49m\u001B[43m=\u001B[49m\u001B[43mverify_integrity\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    391\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcopy\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcopy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    392\u001B[39m \u001B[43m    \u001B[49m\u001B[43msort\u001B[49m\u001B[43m=\u001B[49m\u001B[43msort\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    393\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    395\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m op.get_result()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\core\\reshape\\concat.py:445\u001B[39m, in \u001B[36m_Concatenator.__init__\u001B[39m\u001B[34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001B[39m\n\u001B[32m    442\u001B[39m \u001B[38;5;28mself\u001B[39m.verify_integrity = verify_integrity\n\u001B[32m    443\u001B[39m \u001B[38;5;28mself\u001B[39m.copy = copy\n\u001B[32m--> \u001B[39m\u001B[32m445\u001B[39m objs, keys = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_clean_keys_and_objs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobjs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkeys\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    447\u001B[39m \u001B[38;5;66;03m# figure out what our result ndim is going to be\u001B[39;00m\n\u001B[32m    448\u001B[39m ndims = \u001B[38;5;28mself\u001B[39m._get_ndims(objs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\core\\reshape\\concat.py:507\u001B[39m, in \u001B[36m_Concatenator._clean_keys_and_objs\u001B[39m\u001B[34m(self, objs, keys)\u001B[39m\n\u001B[32m    504\u001B[39m     objs_list = \u001B[38;5;28mlist\u001B[39m(objs)\n\u001B[32m    506\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(objs_list) == \u001B[32m0\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m507\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mNo objects to concatenate\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    509\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m keys \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    510\u001B[39m     objs_list = \u001B[38;5;28mlist\u001B[39m(com.not_none(*objs_list))\n",
      "\u001B[31mValueError\u001B[39m: No objects to concatenate"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Salvando em Parquet\n",
    "\n",
    "\n",
    "Salvamos o resultado consolidado em formato Parquet.\n",
    "Por que Parquet? Ocupa ~80% menos espa√ßo em disco que CSV.\n",
    "Mant√©m os tipos de dados (datas, n√∫meros) corretos, evitando ter que converter tudo de novo na pr√≥xima etapa."
   ],
   "id": "fccad998c391886f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Converte a coluna de data para datetime (essencial para s√©ries temporais)\n",
    "df_final['DT_COMPTC'] = pd.to_datetime(df_final['DT_COMPTC'])\n",
    "\n",
    "# Ordena por Fundo e Data\n",
    "df_final.sort_values(by=['CNPJ_FUNDO', 'DT_COMPTC'], inplace=True)\n",
    "\n",
    "arquivo_saida = f'{PROCESSED_DIR}/base_acoes_consolidada.parquet'\n",
    "print(f\"üíæ Salvando em: {arquivo_saida}\")\n",
    "\n",
    "# Salva em Parquet (requer biblioteca pyarrow ou fastparquet instalada)\n",
    "df_final.to_parquet(arquivo_saida, index=False)\n",
    "\n",
    "print(\"‚úÖ Processo de Limpeza Conclu√≠do!\")"
   ],
   "id": "203aea49afd45340",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
